{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Political Bias Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = pd.read_csv('../../data/accounts.csv', delimiter=';')\n",
    "accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(accounts['spectrum'], return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1 ,1)\n",
    "ax.bar(unique, counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../../data/tweets.csv', delimiter=';')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(tweets['spectrum'], return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1 ,1)\n",
    "ax.bar(unique, counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(tweets['spectrum'])\n",
    "\n",
    "tweets['spectrum'] = le.transform(tweets['spectrum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_tknzr = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_normalized'] = tweets['text'].apply(lambda x: preprocessing(x, tt_tknzr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(tweets.columns[:-2], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_len = tweets.shape[0]\n",
    "tweets.drop_duplicates(inplace=True)\n",
    "print(f'{old_len - tweets.shape[0]} tweets were dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 777\n",
    "\n",
    "df_train, df_test = train_test_split(tweets, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFBertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-large-portuguese-cased'\n",
    "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
    "\n",
    "# bert_model = TFBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, from_pt=True)\n",
    "bert_model = TFBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_tknzr = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 280\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, x_in: np.array, y_in: np.array, tokenizer: transformers.AutoTokenizer, max_len: int, batch_size: int, shuffle: bool=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.x = x_in\n",
    "        self.y = y_in\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datalen = len(y_in)\n",
    "        self.indexes = np.arange(self.datalen)\n",
    "        self.max_len = max_len\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[List[np.array], np.array]:\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        x_batch = self.x[batch_indexes]\n",
    "        y_batch = self.y[batch_indexes]\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        for text in x_batch:\n",
    "            encoded = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        return [np.array(input_ids, dtype=np.int64), np.array(attention_masks, dtype=np.int64)], y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datalen // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.datalen)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "train_gen = DataGenerator(df_train['text_normalized'].to_numpy(), df_train['spectrum'].to_numpy(), bert_tknzr, MAX_LEN, BATCH_SIZE)\n",
    "test_gen = DataGenerator(df_test['text_normalized'].to_numpy(), df_test['spectrum'].to_numpy(), bert_tknzr, MAX_LEN, BATCH_SIZE, shuffle=False)\n",
    "val_gen = DataGenerator(df_val['text_normalized'].to_numpy(), df_val['spectrum'].to_numpy(), bert_tknzr, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(bert_model: TFBertModel) -> TFBertModel:\n",
    "    input_ids = tf.keras.Input(shape=(MAX_LEN,), dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(MAX_LEN,), dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids, attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='./checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen, \n",
    "    epochs=N_EPOCHS, \n",
    "    validation_data=val_gen, \n",
    "    verbose=1, \n",
    "    shuffle=True, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(history.history['accuracy'])\n",
    "accuracy = np.zeros(n + 1)\n",
    "val_accuracy = np.zeros(n + 1)\n",
    "\n",
    "for i in range(n):\n",
    "    accuracy[i + 1] = history.history['accuracy'][i]\n",
    "    val_accuracy[i + 1] = history.history['val_accuracy'][i]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "epochs = [(i + 1) for i in range(n + 1)]\n",
    "\n",
    "ax.plot(\n",
    "    epochs,\n",
    "    accuracy,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    epochs, \n",
    "    val_accuracy\n",
    ")\n",
    "\n",
    "ax.scatter(epochs, accuracy)\n",
    "ax.scatter(epochs, val_accuracy)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "\n",
    "labels = ['Accuracy', 'Validation Accuracy']\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "for i, value in enumerate([accuracy[-1], val_accuracy[-1]]): \n",
    "    ax.text(\n",
    "        n + 1,\n",
    "        value - 0.01,\n",
    "        labels[i],\n",
    "        c=colors[i],\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        size=14\n",
    "    )\n",
    "    \n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1, decimals=0))\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.title('Data split')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_gen)\n",
    "\n",
    "y_pred = [le.inverse_transform(np.argmax(pred).reshape(-1,1))[0] for pred in predictions]\n",
    "cf_matrix = confusion_matrix(le.inverse_transform(df_test['spectrum']), y_pred, normalize='true')\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=set(le.inverse_transform(df_test['spectrum'])))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "disp.plot(ax=ax, xticks_rotation=45)\n",
    "\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "disp.im_.colorbar.remove()\n",
    "disp.ax_.set_xlabel('Predicted label')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(\n",
    "    le.inverse_transform(df_test['spectrum']), y_pred\n",
    ")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(\n",
    "    le.inverse_transform(df_test['spectrum']), y_pred, \n",
    "    labels = list(set(le.inverse_transform(df_test['spectrum']))), \n",
    "    average = 'micro'\n",
    ")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/doodles.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='topology.png',\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e22273a825d950fe4efd21f53b9fd7fac63ac5260aa589673936dd59458a2bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
